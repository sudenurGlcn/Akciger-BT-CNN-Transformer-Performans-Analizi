"""BEiT Model K-Fold Cross Validation

BEiT (Bidirectional Encoder representation from Image Transformers) modeli ile
K-fold cross validation eƒüitimi - CUDA Optimized PyTorch
"""

import os
import numpy as np
import pandas as pd
import cv2
import gc
from tqdm import tqdm
from glob import glob
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torch.cuda.amp import autocast, GradScaler
import torch.nn.functional as F
from torch.multiprocessing import freeze_support
from transformers import BeitFeatureExtractor, BeitForImageClassification
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import json
import pickle

# ============================================
# CUDA GPU AYARLARI VE DURUM KONTROL√ú
# ============================================

# Zaman damgasƒ±
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")


class IQDataset(Dataset):
    def __init__(self, dataframe, transform=None, feature_extractor=None, class_to_idx=None):
        """
        Args:
            dataframe (pd.DataFrame): Veri seti DataFrame'i
            transform (callable, optional): G√∂r√ºnt√º d√∂n√º≈ü√ºmleri
            feature_extractor (callable, optional): BEiT feature extractor
            class_to_idx (dict, optional): Sƒ±nƒ±f isimlerinden indekslere e≈üleme
        """
        self.dataframe = dataframe
        self.transform = transform
        self.feature_extractor = feature_extractor
        self.class_to_idx = class_to_idx

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx]['filepaths']
        label = self.dataframe.iloc[idx]['labels']

        # G√∂r√ºnt√ºy√º oku
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"G√∂r√ºnt√º okunamadƒ±: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        if self.transform:
            image = self.transform(image)

        # BEiT feature extractor ile g√∂r√ºnt√ºy√º i≈üle
        if self.feature_extractor:
            # G√∂r√ºnt√ºy√º PIL formatƒ±na √ßevir
            image_pil = transforms.ToPILImage()(image)
            inputs = self.feature_extractor(images=image_pil, return_tensors="pt")
            # pixel_values'ƒ± al ve squeeze et
            image = inputs['pixel_values'].squeeze(0)

        # Label'ƒ± sayƒ±sal deƒüere d√∂n√º≈üt√ºr
        if self.class_to_idx is not None:
            label_idx = self.class_to_idx[label]
        else:
            label_idx = label

        return image, torch.tensor(label_idx, dtype=torch.long)


def main():
    # CUDA GPU'larƒ± kontrol et
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîç Tespit edilen GPU sayƒ±sƒ±: {torch.cuda.device_count()}")

    # Global deƒüi≈üken - GPU durumu
    global GPU_AVAILABLE, DEVICE_NAME, HARDWARE_TYPE, MIXED_PRECISION
    GPU_AVAILABLE = torch.cuda.is_available()
    DEVICE_NAME = device
    HARDWARE_TYPE = 'CUDA GPU' if GPU_AVAILABLE else 'CPU'

    print(f"üñ•Ô∏è  Kullanƒ±lacak Hardware: {HARDWARE_TYPE}")
    print(f"üéØ Device: {DEVICE_NAME}")

    if GPU_AVAILABLE:
        try:
            # GPU bilgilerini g√∂ster
            gpu_name = torch.cuda.get_device_name(0)
            print(f"üîß GPU: {gpu_name}")
            print(f"üîß CUDA Version: {torch.version.cuda}")

            # Mixed precision training i√ßin scaler
            scaler = GradScaler()
            print("‚úÖ Mixed Precision (AMP) aktif")
            MIXED_PRECISION = True

            # GPU memory bilgisi
            memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            memory_reserved = torch.cuda.memory_reserved(0) / (1024**3)
            print(f"üíæ GPU Memory: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved")

        except RuntimeError as e:
            print(f"‚ùå GPU ayarlarƒ± hatasƒ±: {e}")
            device = torch.device('cpu')
            GPU_AVAILABLE = False
            DEVICE_NAME = device
            HARDWARE_TYPE = 'CPU'
            MIXED_PRECISION = False
    else:
        print("‚ö†Ô∏è  CUDA GPU bulunamadƒ±, CPU kullanƒ±lacak")
        MIXED_PRECISION = False

    # PyTorch CUDA durum kontrol√º
    print(f"\nüîç PyTorch CUDA Durumu:")
    print(f"   - CUDA Available: {torch.cuda.is_available()}")
    print(f"   - CUDA Version: {torch.version.cuda if GPU_AVAILABLE else 'N/A'}")
    print(f"   - Mixed Precision: {'Aktif' if MIXED_PRECISION else 'Pasif'}")

    # Ana dizin (K-fold b√∂l√ºnm√º≈ü veri seti)
    main_directory = r'C:\Users\Derin\PycharmProjects\LungClassification\Datasets\IQ-OTHNCCD_K-fold'

    # Sƒ±nƒ±f isimleri
    class_labels = ['Benign cases', 'Malignant cases', 'Normal cases']  # IQ-OTHNCCD sƒ±nƒ±flarƒ±
    # Sƒ±nƒ±f indekslerini olu≈ütur
    class_to_idx = {label: idx for idx, label in enumerate(class_labels)}
    k_folds = 5

    # Sonu√ß klas√∂rlerini olu≈ütur
    model_dir = r'D:\Grup-1-LungClassification\Models\beit_models-IQ-OTHNCCD'
    results_dir =  r'D:\Grup-1-LungClassification\Models\beit_results_dir-IQ-OTHNCCD'

    # Klas√∂rleri olu≈ütur
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(results_dir, exist_ok=True)

    # Veri d√∂n√º≈ü√ºmleri - optimize edilmi≈ü transform
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    def load_fold_data(fold_dir, split):
        """Belirli bir fold ve split i√ßin veri y√ºkleme - optimize edilmi≈ü"""
        print(f"üìÅ Veri y√ºkleniyor: {split} set - {HARDWARE_TYPE}")
        filepaths = []
        labels = []

        split_dir = os.path.join(fold_dir, split)

        # Paralel i≈üleme i√ßin glob kullan
        for label in class_labels:
            class_dir = os.path.join(split_dir, label)
            if os.path.exists(class_dir):
                # glob ile daha hƒ±zlƒ± dosya listesi olu≈ütur
                jpg_files = glob(os.path.join(class_dir, "*.jpg"))
                filepaths.extend(jpg_files)
                labels.extend([label] * len(jpg_files))

        return pd.DataFrame({
            "filepaths": filepaths,
            "labels": labels
        })

    def train(model, optimizer, train_loader, val_loader, device, scaler=None, model_dir=None, fold=None, epochs=50):
        """Model eƒüitim fonksiyonu - optimize edilmi≈ü"""
        model.train()
        criterion = nn.CrossEntropyLoss()
        history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': []
        }

        best_val_loss = float('inf')
        patience = 5
        patience_counter = 0

        # Model'i train moduna al ve gradyanlarƒ± optimize et
        model.train()
        torch.backends.cudnn.benchmark = True  # CUDA optimizasyonu

        for epoch in range(epochs):
            # Training
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0

            train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')
            for batch in train_bar:
                inputs, labels = batch
                inputs = inputs.to(device, non_blocking=True)  # non_blocking=True ile asenkron transfer
                labels = labels.to(device, non_blocking=True)

                optimizer.zero_grad(set_to_none=True)  # Daha hƒ±zlƒ± sƒ±fƒ±rlama

                if GPU_AVAILABLE and MIXED_PRECISION and scaler is not None:
                    with autocast():
                        outputs = model(pixel_values=inputs).logits
                        loss = criterion(outputs, labels)

                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(pixel_values=inputs).logits
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()

                with torch.no_grad():  # Gereksiz gradyan hesaplamalarƒ±nƒ± √∂nle
                    train_loss += loss.item()
                    _, predicted = outputs.max(1)
                    train_total += labels.size(0)
                    train_correct += predicted.eq(labels).sum().item()

                train_bar.set_postfix({
                    'loss': f'{train_loss/train_total:.4f}',
                    'acc': f'{100.*train_correct/train_total:.2f}%'
                })

            train_loss = train_loss / len(train_loader)
            train_acc = train_correct / train_total

            # Validation 
            model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0

            with torch.no_grad(), torch.cuda.amp.autocast(enabled=GPU_AVAILABLE and MIXED_PRECISION):
                val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')
                for batch in val_bar:
                    inputs, labels = batch
                    inputs = inputs.to(device, non_blocking=True)
                    labels = labels.to(device, non_blocking=True)

                    outputs = model(pixel_values=inputs).logits
                    loss = criterion(outputs, labels)

                    val_loss += loss.item()
                    _, predicted = outputs.max(1)
                    val_total += labels.size(0)
                    val_correct += predicted.eq(labels).sum().item()

                    val_bar.set_postfix({
                        'loss': f'{val_loss/val_total:.4f}',
                        'acc': f'{100.*val_correct/val_total:.2f}%'
                    })

            val_loss = val_loss / len(val_loader)
            val_acc = val_correct / val_total

            # History'ye kaydet
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)

            # Early stopping kontrol√º
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # En iyi modeli kaydet
                if model_dir is not None and fold is not None:
                    torch.save(model.state_dict(), os.path.join(model_dir, f'best_model_fold_{fold}.pt'))
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"\nEarly stopping triggered at epoch {epoch+1}")
                    break

            print(f"\nEpoch {epoch+1}/{epochs}:")
            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

        return history

    def evaluate(model, test_loader, device):
        """Model deƒüerlendirme fonksiyonu"""
        model.eval()
        criterion = nn.CrossEntropyLoss()
        test_loss = 0
        test_correct = 0
        test_total = 0

        with torch.no_grad():
            test_bar = tqdm(test_loader, desc='Evaluating')
            for batch in test_bar:
                inputs, labels = batch
                inputs = inputs.to(device)
                labels = labels.to(device)

                if GPU_AVAILABLE and MIXED_PRECISION:
                    with autocast():
                        outputs = model(pixel_values=inputs).logits
                        loss = criterion(outputs, labels)
                else:
                    outputs = model(pixel_values=inputs).logits
                    loss = criterion(outputs, labels)

                test_loss += loss.item()
                _, predicted = outputs.max(1)
                test_total += labels.size(0)
                test_correct += predicted.eq(labels).sum().item()

                test_bar.set_postfix({
                    'loss': f'{test_loss/test_total:.4f}',
                    'acc': f'{100.*test_correct/test_total:.2f}%'
                })

        test_loss = test_loss / len(test_loader)
        test_acc = test_correct / test_total

        return test_loss, test_acc

    def predict(model, test_loader, device):
        """Model tahmin fonksiyonu"""
        model.eval()
        predictions = []

        with torch.no_grad():
            test_bar = tqdm(test_loader, desc='Predicting')
            for batch in test_bar:
                inputs, _ = batch
                inputs = inputs.to(device)

                if GPU_AVAILABLE and MIXED_PRECISION:
                    with autocast():
                        outputs = model(pixel_values=inputs).logits
                else:
                    outputs = model(pixel_values=inputs).logits

                predictions.append(outputs.cpu().numpy())

        return np.vstack(predictions)

    # K-fold cross validation
    fold_results = []
    all_histories = []

    print(f"\n{'=' * 60}")
    print(f"üöÄ K-FOLD CROSS VALIDATION BA≈ûLIYOR")
    print(f"üìç Hardware: {HARDWARE_TYPE}")
    print(f"üéØ Device: {DEVICE_NAME}")
    if GPU_AVAILABLE:
        print(f"üî• Mixed Precision: {'Aktif' if MIXED_PRECISION else 'Pasif'}")
    print(f"{'=' * 60}\n")

    for fold in range(1, k_folds + 1):
        print(f"\n{'=' * 50}")
        print(f"üìÇ FOLD {fold}/{k_folds}")
        print(f"üñ•Ô∏è  Hardware: {HARDWARE_TYPE}")
        print(f"{'=' * 60}")

        # CUDA memory temizleme
        if GPU_AVAILABLE:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            print(f"üíæ Pre-fold GPU Memory: {torch.cuda.memory_allocated(0) / (1024 ** 3):.2f}GB allocated, {torch.cuda.memory_reserved(0) / (1024 ** 3):.2f}GB reserved")
        else:
            print(f"üñ•Ô∏è  CPU mode - Fold {fold}")

        # Fold dizinini belirle
        fold_dir = os.path.join(main_directory, f'fold_{fold}')

        # Train ve test verilerini y√ºkle
        train_df = load_fold_data(fold_dir, 'train')
        test_df = load_fold_data(fold_dir, 'test')

        # Eƒüer veri yoksa atla
        if len(train_df) == 0 or len(test_df) == 0:
            print(f"UYARI: Fold {fold} i√ßin veri bulunamadƒ±, bu fold atlanƒ±yor...")
            continue

        # Train setini train ve validation olarak b√∂l (80-20)
        train_set, val_set = train_test_split(
            train_df,
            test_size=0.2,
            random_state=42,
            stratify=train_df['labels']
        )

        print(f"Train √∂rnekleri: {len(train_set)}")
        print(f"Validation √∂rnekleri: {len(val_set)}")
        print(f"Test √∂rnekleri: {len(test_df)}")

        # Sabit batch size
        batch_size = 32
        print(f"üìä Batch size: {batch_size}")

        # BEiT model ve feature extractor'ƒ± y√ºkle
        print("\nüèóÔ∏è  BEiT model y√ºkleniyor...")
        feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224')
        model = BeitForImageClassification.from_pretrained(
            'microsoft/beit-base-patch16-224',
            num_labels=len(class_labels),
            ignore_mismatched_sizes=True
        )
        model = model.to(device)
        print("‚úÖ BEiT model y√ºklendi")

        # DataLoader'larƒ± olu≈ütur - optimize edilmi≈ü
        train_loader = DataLoader(
            IQDataset(train_set, transform, feature_extractor, class_to_idx),
            batch_size=batch_size,
            shuffle=True,
            num_workers=min(8, os.cpu_count()),  # CPU √ßekirdek sayƒ±sƒ±na g√∂re optimize et
            pin_memory=True,
            persistent_workers=True,  # Worker'larƒ± yeniden kullan
            prefetch_factor=2  # Veri √∂n y√ºkleme
        )

        val_loader = DataLoader(
            IQDataset(val_set, transform, feature_extractor, class_to_idx),
            batch_size=batch_size,
            shuffle=False,
            num_workers=min(8, os.cpu_count()),
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )

        test_loader = DataLoader(
            IQDataset(test_df, transform, feature_extractor, class_to_idx),
            batch_size=batch_size,
            shuffle=False,
            num_workers=min(8, os.cpu_count()),
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )

        # Optimizer ve loss function
        optimizer = optim.Adamax(model.parameters(), lr=0.0004, weight_decay=0.01)

        # Model eƒüitimi
        print(f"\nüöÄ Fold {fold} eƒüitimi ba≈ülƒ±yor...")
        print(f"üìç Device: {DEVICE_NAME} ({HARDWARE_TYPE})")
        if GPU_AVAILABLE:
            print(f"üî• GPU accelerated training aktif")
            print(f"‚ö° Mixed Precision: {'Aktif' if MIXED_PRECISION else 'Pasif'}")

        history = train(model, optimizer, train_loader, val_loader, device,
                      scaler if GPU_AVAILABLE else None,
                      model_dir=model_dir,
                      fold=fold,
                      epochs=50)

        # Test deƒüerlendirmesi
        print(f"\nüìä Fold {fold} test deƒüerlendirmesi... ({HARDWARE_TYPE})")
        test_loss, test_acc = evaluate(model, test_loader, device)

        # Tahmin yapma
        print(f"üîÆ Tahmin yapƒ±lƒ±yor... ({HARDWARE_TYPE})")
        predictions = predict(model, test_loader, device)
        y_pred = np.argmax(predictions, axis=1)

        # True labels'ƒ± al
        y_true = []
        for _, labels in test_loader:
            y_true.extend(labels.cpu().numpy())

        # Sƒ±nƒ±f isimlerini al
        y_pred_labels = [class_labels[i] for i in y_pred]
        y_true_labels = [class_labels[i] for i in y_true]

        # Fold sonu√ßlarƒ±nƒ± kaydet
        fold_result = {
            'fold': fold,
            'test_accuracy': test_acc,
            'test_loss': test_loss,
            'y_true': y_true_labels,
            'y_pred': y_pred_labels
        }

        print(f"\nüíæ Fold {fold} sonu√ßlarƒ± kaydediliyor...")
        save_fold_results(fold, history, fold_result, results_dir, timestamp, class_labels)
        print(f"‚úÖ Fold {fold} sonu√ßlarƒ± kaydedildi!")

        # Sonu√ßlarƒ± listeye ekle
        fold_results.append(fold_result)
        all_histories.append(history)

        print(f"\nüìä Fold {fold} Sonu√ßlarƒ±:")
        print(f"Test Accuracy: {test_acc:.4f}")
        print(f"Test Loss: {test_loss:.4f}")
        print(f"Eƒüitim tamamlandƒ±ƒüƒ± epoch: {len(history['train_loss'])}")

        # Classification report
        print(f"\nüìã Fold {fold} Classification Report:")
        print(classification_report(y_true_labels, y_pred_labels))

        # Model kaydet
        model_path = os.path.join(model_dir, f'beit_model_fold_{fold}.pt')
        torch.save(model.state_dict(), model_path)
        print(f"Model kaydedildi: {model_path}")

        # Memory temizleme
        del model
        if GPU_AVAILABLE:
            torch.cuda.empty_cache()
            print(f"üßπ GPU memory temizlendi")
        gc.collect()

        print(f"‚úÖ Fold {fold} tamamlandƒ±! ({HARDWARE_TYPE})")

    # Final GPU memory durumu
    if GPU_AVAILABLE:
        memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)
        memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)
        print(f"\nüíæ Final GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved")
        print(f"üî• GPU accelerated training tamamlandƒ±!")
    else:
        print(f"\nüñ•Ô∏è  CPU training tamamlandƒ±!")

    # Final √∂zet sonu√ßlarƒ± kaydet
    print("\nüíæ Final √∂zet sonu√ßlar kaydediliyor...")
    summary = {
        'timestamp': timestamp,
        'total_folds': len(fold_results),
        'average_accuracy': float(np.mean([r['test_accuracy'] for r in fold_results])),
        'std_accuracy': float(np.std([r['test_accuracy'] for r in fold_results])),
        'average_loss': float(np.mean([r['test_loss'] for r in fold_results])),
        'std_loss': float(np.std([r['test_loss'] for r in fold_results]))
    }

    with open(os.path.join(results_dir, f'summary_{timestamp}.json'), 'w') as f:
        json.dump(summary, f, indent=4)
    print("‚úÖ Final √∂zet sonu√ßlar kaydedildi!")

    print(f"\n{'=' * 60}")
    print(f"üéâ K-fold cross validation tamamlandƒ±!")
    print(f"üìç Kullanƒ±lan Hardware: {HARDWARE_TYPE}")
    print(f"üéØ Device: {DEVICE_NAME}")
    if GPU_AVAILABLE:
        print(f"‚ö° Mixed Precision: {'Aktif' if MIXED_PRECISION else 'Pasif'}")
    print(f"{'=' * 60}")

    print(f"\nSONU√áLAR KAYDEDƒ∞LDƒ∞:")
    print(f"- Modeller: {model_dir}/ klas√∂r√ºnde")
    print(f"- Grafikler ve sonu√ßlar: {results_dir}/ klas√∂r√ºnde")
    print(f"- Zaman damgasƒ±: {timestamp}")
    print(f"- Optimizer: Adamax (Learning Rate: 0.0004)")
    print(f"- Early Stopping: Aktif (patience=5)")
    print(f"- Hardware: {HARDWARE_TYPE}")


def save_fold_results(fold, history, fold_result, results_dir, timestamp, class_labels):
    """Tek bir fold'un sonu√ßlarƒ±nƒ± kaydetme fonksiyonu"""
    fold_dir = os.path.join(results_dir, f'fold_{fold}')
    os.makedirs(fold_dir, exist_ok=True)

    # Fold sonu√ßlarƒ±nƒ± JSON formatƒ±nda kaydet
    results_file = os.path.join(fold_dir, f'fold_{fold}_results_{timestamp}.json')
    with open(results_file, 'w') as f:
        json.dump({
            'fold': fold,
            'test_accuracy': float(fold_result['test_accuracy']),
            'test_loss': float(fold_result['test_loss']),
            'y_true': fold_result['y_true'],
            'y_pred': fold_result['y_pred']
        }, f, indent=4)

    # Eƒüitim ge√ßmi≈üini pickle formatƒ±nda kaydet
    history_file = os.path.join(fold_dir, f'fold_{fold}_history_{timestamp}.pkl')
    with open(history_file, 'wb') as f:
        pickle.dump(history, f)

    # Loss ve accuracy grafiƒüi
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title(f'Fold {fold} - Loss History')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Train Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.title(f'Fold {fold} - Accuracy History')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(fold_dir, f'fold_{fold}_history_{timestamp}.png'))
    plt.close()

    # Confusion matrix
    y_true = fold_result['y_true']
    y_pred = fold_result['y_pred']
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels,
                yticklabels=class_labels)
    plt.title(f'Fold {fold} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.tight_layout()
    plt.savefig(os.path.join(fold_dir, f'fold_{fold}_confusion_matrix_{timestamp}.png'))
    plt.close()


if __name__ == '__main__':
    freeze_support()
    main()